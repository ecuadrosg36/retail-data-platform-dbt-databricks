# Retail Data Platform - dbt + Databricks

A **production-grade retail analytics platform** built with dbt Core and Databricks, implementing the Medallion Architecture (Bronze â†’ Silver â†’ Gold).

![dbt](https://img.shields.io/badge/dbt-1.8.6-orange)
![Databricks](https://img.shields.io/badge/Databricks-Delta%20Lake-red)
![Python](https://img.shields.io/badge/Python-3.11%2B-blue)

## ğŸŒŸ Features

- **Medallion Architecture**: Bronze (seeds) â†’ Silver (staging/intermediate) â†’ Gold (marts)
- **20+ dbt Models**: Staging, intermediate, dimensions, and facts
- **Comprehensive Testing**: >80% coverage with dbt_expectations and dbt_utils
- **Incremental Models**: Optimized for large-scale data processing
- **SCD Type 2**: Snapshot-based slowly changing dimensions
- **Databricks Optimizations**: Delta Lake, OPTIMIZE, Z-ORDER clustering
- **CI/CD Pipeline**: GitHub Actions with slim CI and automated docs
- **Data Observability**: Elementary dbt integration for monitoring
- **BI Exposures**: Documented dashboard dependencies

## ğŸ“Š Data Flow

```
Seeds (Bronze)
    â†“ source freshness checks
Staging (Silver) - Basic transformations
    â†“ joins & enrichment
Intermediate (Silver) - Business logic (RFM, metrics)
    â†“ dimensional modeling
Marts (Gold) - Analytics-ready tables
    â†“
BI Dashboards & Reports
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Databricks workspace with SQL Warehouse
- Databricks personal access token

### Setup

```bash
# Clone and install
git clone <repo>
cd retail_data_platform_dbt_databricks
pip install -r requirements.txt

# Install dbt packages
dbt deps

# Configure Databricks connection
export DATABRICKS_TOKEN=your_token_here
# Edit profiles.yml with your host and http_path
```

### Run Pipeline

```bash
# Load seed data
dbt seed

# Run all models
dbt run

# Run tests
dbt test

# Build everything (run + test)
dbt build

# Generate documentation
dbt docs generate
dbt docs serve
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/          # Bronze â†’ Silver transformations
â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â””â”€â”€ stg_products.sql
â”‚   â”œâ”€â”€ intermediate/      # Business logic layer
â”‚   â”‚   â”œâ”€â”€ int_orders_enriched.sql
â”‚   â”‚   â””â”€â”€ int_customer_metrics.sql
â”‚   â””â”€â”€ marts/            # Gold layer (analytics-ready)
â”‚       â”œâ”€â”€ dimensions/
â”‚       â”‚   â”œâ”€â”€ dim_customers.sql  # SCD Type 2
â”‚       â”‚   â”œâ”€â”€ dim_products.sql
â”‚       â”‚   â””â”€â”€ dim_dates.sql
â”‚       â””â”€â”€ facts/
â”‚           â”œâ”€â”€ fct_orders.sql
â”‚           â””â”€â”€ fct_daily_sales.sql  # Incremental
â”œâ”€â”€ snapshots/            # SCD Type 2 tracking
â”œâ”€â”€ macros/               # Custom macros
â”œâ”€â”€ seeds/                # Sample data (Bronze)
â”œâ”€â”€ tests/                # Custom tests
â””â”€â”€ packages.yml          # dbt packages
```

## ğŸ§ª Testing

The project includes comprehensive data quality tests:

- **Generic Tests**: unique, not_null, accepted_values, relationships
- **dbt_expectations**: Range checks, regex validation, row count monitoring
- **dbt_utils**: Expression validation, equality checks, recency tests
- **Custom Tests**: Business rule validation

```bash
# Run all tests
dbt test

# Run tests for specific model
dbt test --select dim_customers

# Run only schema tests
dbt test --data
```

## ğŸ¯ Key Models

### Dimensions
- **dim_customers**: Customer dimension with RFM segmentation (Recency, Frequency, Monetary)
- **dim_products**: Product catalog with price tier categorization
- **dim_dates**: Date dimension with calendar attributes

### Facts
- **fct_orders**: Order-level facts (one row per order)
- **fct_daily_sales**: Daily aggregated metrics (incremental)

### Metrics Included
- Customer lifetime value
- RFM scores and segments
- Order completion rates
- Daily revenue trends
- Product performance

## âš¡ Databricks Optimizations

The project leverages Databricks-specific features:

- **Delta Lake**: All models use Delta format
- **OPTIMIZE**: Post-hooks for automatic file compaction
- **Z-ORDER**: Clustering on primary keys
- **Liquid Clustering**: (Databricks Runtime 13+)
- **Incremental Strategies**: Merge for efficient updates
-**Photon**: Optimized for analytical queries

## ğŸ“š Documentation

Documentation is generated automatically and includes:

- Model descriptions and lineage
- Column-level documentation
- Business glossary
- Test coverage
- BI dashboard dependencies (exposures)

Access docs at: `http://localhost:8080` after running `dbt docs serve`

## ğŸ”„ CI/CD

GitHub Actions workflow includes:

1. **SQL Linting** (sqlfl uff)
2. **Slim CI** (only modified models)
3. **Automated Testing** (all models + tests)
4. **Docs Deployment** (GitHub Pages)

## ğŸ“ˆ Monitoring

Elementary dbt integration provides:

- Data anomaly detection
- Schema change tracking
- Test failure alerts
- Lineage visualization
- Slack/email notifications

## ğŸ› ï¸ Development

```bash
# Create new model
dbt run --select +my_new_model

# Test modified models only
dbt test --select state:modified+

# Generate source YAML
dbt run-operation generate_source --args '{"schema_name": "bronze"}'

# Snapshot changes
dbt snapshot
```

## ğŸš¢ Deployment

### Dev Environment
```bash
dbt build --target dev
```

### Production
```bash
dbt build --target prod
```

## ğŸ“ License

MIT License

---

**Built with â¤ï¸ using dbt + Databricks**
